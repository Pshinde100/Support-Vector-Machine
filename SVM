#SUPPORT VECTOR MACHINES#

set.seed(1)
##Simulate our own test data##
x <- matrix(rnorm(20*2), ncol = 2)
y <- c(rep(-1,10), rep(1,10))
x[y == 1,] = x[y == 1, ] + 1

plot(x, col = (3-y))

library(e1071)
dat <- data.frame(x = x, y = as.factor(y))
dat
svm.fit <- svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = F)
plot(svm.fit, dat)
summary(svm.fit)

svm.fit2 <- svm(y ~ ., data = dat, kernel = "linear", cost = 0.1, scale = F)
plot(svm.fit2, dat)
summary(svm.fit2)

##Cross validation to select the best hyperparameter and model##
set.seed(1)
tune.out <- tune(svm, y~., data = dat, kernel = "linear", 
                 ranges = list(cost = c(0.001, 0.01, 0.1,1,5,10,100)))
summary(tune.out)
tune.out$best.model

##predicting the values for test set using the best model##
xtest = matrix(rnorm(20*2), ncol = 2)
ytest = sample(c(1,-1), 20, rep = T)
xtest[ytest ==1,] = xtest[ytest ==1,] + 1
testdat = data.frame(x = xtest, y = as.factor(ytest))
 
yhat <- predict(tune.out$best.model, testdat)
library(caret)

confusionMatrix(yhat, testdat$y)

##Changing Kernels for SVM##

###generate some test data###
set.seed(1)
x <- matrix(rnorm(200*2), ncol = 2)
x[1:100,] = x[1:100,] + 2 
x[101:150,] = x[101:150,] - 2

y <- c(rep(1,150), rep(2,50))
dat <- data.frame(x = x, y = as.factor(y))
plot(x, col = y)

###randomly split the data into training and test sets and fit a radial kernel###
train <- sample(200, 100)
svm.fit <- svm(y~. , data = dat[train,], kernel = "radial", gamma = 1, 
               cost = 1) 
plot(svm.fit, dat[train,])
summary(svm.fit)
yhat <- predict(svm.fit, dat[-train,])
confusionMatrix(yhat, dat[-train, 'y'])

#we can see from the figure that there are a fair number of training errors
#for this fit. If we increase the value of cost, we can reduce the training
#errors, but we risk overfitting the data

svm.fit <- svm(y~., dat[train,], kernel = 'radial', gamma = 1, cost = 1e5)
plot(svm.fit, dat[train,])
#This is certainly very irregular, and probably would overfit the data, lets
#try cross validating these parameters instead

set.seed(1)
tune.out <- tune(svm, y~., data = dat[train,], kernel = 'radial',
                 ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                 gamma = c(0.5, 1, 2, 3, 4)))
tune.out
summary(tune.out)
summary(tune.out$best.model)
tune.out$best.model
yhat <- predict(tune.out$best.model, dat[-train,])
confusionMatrix(yhat, dat[-train, 'y'])

##ROC plot for SVM##
library(ROCR)

#while building a svm, in order to obtain a roc plot it is necessary for us
#that we set the 'decision.values = T' to obtain the fitted values
svm.opt <- svm(y~., data = dat[train,], kernel = 'radial', gamma = 2,
               cost = 1, decision.values = T)
fitted <- attributes(predict(svm.opt, dat[train,], 
                             
                             
                             decision.values = T))$decision.values
rocplot(fitted, dat[train,'y'], main = 'trainingdata')

##function for ROC plot##
rocplot <- function(pred, truth, ...){
        predob = prediction(pred, truth)
        perf = performance(predob, 'tpr', 'fpr')
        plot(perf, ...)
}
